{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D, Dense, Dropout, Flatten, LeakyReLU, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/UrbanSound8K/\")\n",
    "AUDIO_DIR = DATA_DIR / \"audio\"\n",
    "META_CSV = DATA_DIR / \"metadata\" / \"UrbanSound8K.csv\"\n",
    "NUM_CLASSES = 10\n",
    "SAMPLING_RATE = 16000\n",
    "INPUT_SIZE = 16000\n",
    "STRIDE = 8000\n",
    "\n",
    "WEIGHT_DECAY = 1e-4 # l2 regularization hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(META_CSV)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunks(X, y, chunk_size, stride):\n",
    "    \"\"\"Split a numpy array into chunks of given size jumping stride indices each time.\n",
    "    Any chunks of smaller size are padded with 0 at the end.\"\"\"\n",
    "    chunks = []\n",
    "    for start in range(0, len(X), stride):\n",
    "        chunk = X[start : start + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk)\n",
    "        else:\n",
    "            chunk = np.pad(chunk, (0, chunk_size - len(chunk)))\n",
    "            chunks.append(chunk)\n",
    "            break\n",
    "    #chunks = [X[end - chunk_size : end] for end in range(chunk_size, len(X)+1, stride)]\n",
    "    #X = np.array([np.pad(chunk, (0, chunk_size - len(chunk))) for chunk in chunks])\n",
    "    y = np.repeat(y, len(chunks))\n",
    "    return np.array(chunks), y, len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fold_data(meta, fold):\n",
    "    \"\"\"Load the audio and label data for given fold\"\"\"\n",
    "    entries = meta[meta[\"fold\"] == fold]\n",
    "    fold_dir = AUDIO_DIR / f\"fold{fold}\"\n",
    "    filenames = [fold_dir / filename for filename in entries[\"slice_file_name\"]]\n",
    "    audio = [librosa.load(filename, sr=SAMPLING_RATE)[0] for filename in filenames]\n",
    "    classes = entries[\"classID\"]\n",
    "\n",
    "    X, y, chunk_lens = zip(*[to_chunks(x, y, INPUT_SIZE, STRIDE) for x, y in zip(audio, classes)])\n",
    "    X, y = np.concatenate(X), np.concatenate(y)\n",
    "\n",
    "    X = X[..., np.newaxis]  # add new axis required by tensorflow\n",
    "    y = to_categorical(y, num_classes=NUM_CLASSES)  # convert to one-hot encoding\n",
    "    return X, y, np.array(chunk_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run this cell only if the pickled data is in some way incorrect\n",
    "\n",
    "#data = zip(*[load_fold_data(meta, fold) for fold in range(1, 11)])\n",
    "#with (DATA_DIR / \"foldData.pickle\").open(\"bw\") as f:\n",
    "#    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec853efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with (DATA_DIR / \"foldData.pickle\").open(\"br\") as f:\n",
    "    fold_Xs, fold_ys, fold_chunk_lens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071530de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPUs available\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices(gpus[1:], \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ab6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    # CONV1\n",
    "    model.add(Conv1D(16, kernel_size=64, activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "    # CONV2\n",
    "    model.add(Conv1D(32, kernel_size=32, strides=2, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "    # CONV3\n",
    "    model.add(Conv1D(64, kernel_size=16, strides=2, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # CONV4\n",
    "    model.add(Conv1D(128, kernel_size=8, strides=2, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    # CONV1\n",
    "    model.add(Conv1D(16, kernel_size=64, strides=2, activation=\"relu\",\n",
    "                     #kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "    # CONV2\n",
    "    model.add(Conv1D(32, kernel_size=32, strides=2, activation=\"relu\",\n",
    "                    #kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "                    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "    # CONV3\n",
    "    model.add(Conv1D(64, kernel_size=16, strides=2, activation=\"relu\",\n",
    "                    #kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "                    ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # CONV4\n",
    "    model.add(Conv1D(128, kernel_size=8, strides=2, activation=\"relu\",\n",
    "                    #kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "                    ))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97aea1-2200-47bb-86c8-1070e1faf3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mlp(input_shape):\n",
    "    model = Sequential()\n",
    "    # CONV1\n",
    "    #model.add(Conv1D(16, kernel_size=64, strides=2, activation=\"relu\", input_shape=input_shape))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "    # CONV2\n",
    "    #model.add(Conv1D(32, kernel_size=32, strides=2, activation=\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "    # CONV3\n",
    "    #model.add(Conv1D(64, kernel_size=16, strides=2, activation=\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    # CONV4\n",
    "    #model.add(Conv1D(128, kernel_size=8, strides=2, activation=\"relu\"))\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    # FC\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8811b46-6ac5-4f26-92c8-ab06518c7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    # CONV1\n",
    "    model.add(Conv1D(16, kernel_size=16, strides=8,\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # CONV2\n",
    "    #model.add(ZeroPadding1D(padding=1))\n",
    "    model.add(Conv1D(16, kernel_size=6, strides=1))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # CONV3\n",
    "    #model.add(ZeroPadding1D(padding=1))\n",
    "    model.add(Conv1D(32, kernel_size=6, strides=2))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # CONV4\n",
    "    #model.add(ZeroPadding1D(padding=1))\n",
    "    model.add(Conv1D(64, kernel_size=6, strides=2))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # CONV5\n",
    "    #model.add(ZeroPadding1D(padding=1))\n",
    "    model.add(Conv1D(128, kernel_size=6, strides=2))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # CONV6\n",
    "    #model.add(ZeroPadding1D(padding=1))\n",
    "    model.add(Conv1D(128, kernel_size=3, strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01213f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.concatenate(fold_Xs[:9]), np.concatenate(fold_ys[:9])\n",
    "X_test, y_test = fold_Xs[9], fold_ys[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "## CONV1\n",
    "#model.add(\n",
    "#    Conv1D(16, kernel_size=64, strides=2, activation=\"relu\", input_shape=X_train.shape[1:])\n",
    "#)\n",
    "## POOL\n",
    "#model.add(MaxPooling1D(pool_size=8))\n",
    "## CONV2\n",
    "#model.add(\n",
    "#    Conv1D(\n",
    "#        16, kernel_size=64, strides=2, activation=\"relu\", input_shape=(None, INPUT_SIZE)\n",
    "#    )\n",
    "#)\n",
    "## POOL\n",
    "#model.add(MaxPooling1D(pool_size=8))\n",
    "## FC7\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(10, activation=\"softmax\"))\n",
    "#\n",
    "#model.summary()\n",
    "## compile with categorical crossentropy since\n",
    "## this is multi-class classification\n",
    "#model.compile(\n",
    "#    loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19335475",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e57667-8c74-4416-907b-55b4ada7e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=X_train.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1aba2f-6d23-4aec-ba45-cbef532e0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=64, activation='relu', input_shape=(X_train.shape[1:])))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9fa2b-7f58-43cf-8fc4-1bdf4a655050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# CONV1\n",
    "model.add(Conv1D(16, kernel_size=64, strides=2, activation=\"relu\", input_shape=X_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "# CONV2\n",
    "model.add(Conv1D(32, kernel_size=32, strides=2, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=8, strides=8))\n",
    "\n",
    "# CONV3\n",
    "model.add(Conv1D(64, kernel_size=16, strides=2, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# CONV4\n",
    "model.add(Conv1D(128, kernel_size=8, strides=2, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# FC\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ef48e-b0e4-41b9-91a3-71f1d5499225",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87744c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(#optimizer=tf.keras.optimizers.Adadelta(learning_rate=1),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              #loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks to save and stop early\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.weights.best.hdf5\", save_best_only=True, verbose=1)\n",
    "escallback = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do training and save weights to disk\n",
    "hist = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=1/9,\n",
    "    callbacks=[checkpointer, escallback],\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ec263-44bf-478b-aef8-1b8248857522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(hist.history[\"loss\"], label=\"train\")\n",
    "pyplot.plot(hist.history[\"val_loss\"], label=\"test\")\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339baee-8a44-47cc-9c77-b2e249915f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do training and save weights to disk\n",
    "hist = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    validation_split=1/9,\n",
    "    callbacks=[checkpointer, escallback],\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66608ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f467db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "model.load_weights(\"model.weights.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_pred.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3454417",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum(axis=0)/y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = np.array([res.mean(axis=0).argmax() for res in np.split(y_pred, fold_chunk_lens[9].cumsum()[:-1])])\n",
    "y_test_final = np.array([res.mean(axis=0).argmax() for res in np.split(y_test, fold_chunk_lens[9].cumsum()[:-1])])\n",
    "(y_pred_final == y_test_final).sum()/len(y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec60225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"\\n\", \"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac621f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d662d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_idx in range(10):\n",
    "    X_test, y_test = fold_Xs[val_idx], fold_ys[val_idx]\n",
    "    X_train = np.concatenate([fold_Xs[i] for i in range(10) if i != val_idx])\n",
    "    y_train = np.concatenate([fold_ys[i] for i in range(10) if i != val_idx])\n",
    "\n",
    "    model = create_model(input_shape=X_train.shape[1:])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1),\n",
    "                  loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    # callbacks to save and stop early\n",
    "    checkpointer = ModelCheckpoint(filepath=f\"model.weights.best{val_idx}.hdf5\", save_best_only=True, verbose=1)\n",
    "    escallback = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=30, verbose=1)\n",
    "    # do training and save weights to disk\n",
    "    hist = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=100,\n",
    "        epochs=100,\n",
    "        validation_split=1/9,\n",
    "        callbacks=[checkpointer, escallback],\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    # load weights\n",
    "    #model.load_weights(f\"model.weights.best{val_idx}.hdf5\")\n",
    "\n",
    "    model.save(f\"model{val_idx}\")\n",
    "    \n",
    "    # evaluate on test set\n",
    "    score = model.evaluate(X_test, y_test, verbose=1)[1]\n",
    "    scores[val_idx] = score\n",
    "    print(\"\\n\", \"Test accuracy:\", score)\n",
    "\n",
    "    # predictions on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_final = np.array([res.mean(axis=0).argmax() for res in\n",
    "                             np.split(y_pred, fold_chunk_lens[val_idx].cumsum()[:-1])])\n",
    "    y_test_final = np.array([res.mean(axis=0).argmax() for res in\n",
    "                             np.split(y_test, fold_chunk_lens[val_idx].cumsum()[:-1])])\n",
    "    score_final = (y_pred_final == y_test_final).sum()/len(y_pred_final)\n",
    "    print(\"\\n\", \"Final Test accuracy:\", score_final)\n",
    "\n",
    "    scores[val_idx] = (hist, score, score_final, y_pred_final, y_test_final)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a615dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_idx in range(10):\n",
    "    X_test, y_test = fold_Xs[val_idx], fold_ys[val_idx]\n",
    "    #X_train = np.concatenate([fold_Xs[i] for i in range(10) if i != val_idx])\n",
    "    #y_train = np.concatenate([fold_ys[i] for i in range(10) if i != val_idx])\n",
    "\n",
    "    model = keras.models.load_model(f\"model{val_idx}\")\n",
    "    \n",
    "    # evaluate on test set\n",
    "    score = model.evaluate(X_test, y_test, verbose=1)[1]\n",
    "    #scores[val_idx] = score\n",
    "    print(\"\\n\", val_idx, \"Test accuracy:\", score)\n",
    "\n",
    "    # predictions on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_final = np.array([res.mean(axis=0).argmax() for res in\n",
    "                             np.split(y_pred, fold_chunk_lens[val_idx].cumsum()[:-1])])\n",
    "    y_test_final = np.array([res.mean(axis=0).argmax() for res in\n",
    "                             np.split(y_test, fold_chunk_lens[val_idx].cumsum()[:-1])])\n",
    "    score_final = (y_pred_final == y_test_final).sum()/len(y_pred_final)\n",
    "    print(\"\\n\", \"Final Test accuracy:\", score_final)\n",
    "\n",
    "    #scores[val_idx] = (hist, score, score_final, y_pred_final, y_test_final)\n",
    "    #print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_final = np.array([res.mean(axis=0).argmax() for res in\n",
    "                         np.split(y_pred, fold_chunk_lens[val_idx].cumsum()[:-1])])\n",
    "y_test_final = np.array([res.mean(axis=0).argmax() for res in\n",
    "                         np.split(y_test, fold_chunk_lens[val_idx].cumsum()[:-1])])\n",
    "score_final = (y_pred_final == y_test_final).sum()/len(y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cac5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.confusion_matrix(y_test_final, y_pred_final, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(hist.history[\"accuracy\"], label=\"train\")\n",
    "pyplot.plot(hist.history[\"val_accuracy\"], label=\"test\")\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69658f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = tf.math.confusion_matrix(y_test_final, y_pred_final, num_classes=NUM_CLASSES)\n",
    "df_cm = pd.DataFrame(array, index = [i for i in range(10)],\n",
    "                  columns = [i for i in range(10)])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tf.math.confusion_matrix(y_test_final, y_pred_final, num_classes=NUM_CLASSES).numpy()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.delete(res, 2, 0)\n",
    "res = np.delete(res, 2, 1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.diagonal().sum()/res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12790d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
